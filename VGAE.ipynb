{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# os.environ['TORCH'] = torch.__version__\n",
    "# # print(torch.__version__)\n",
    "\n",
    "# # !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# # !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# # !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naveenmalla/Documents/Projects/Thesis/Code/Synthetic_City_Generator/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naveenmalla/Documents/Projects/Thesis/Code/Synthetic_City_Generator/venv/lib/python3.12/site-packages/torch_geometric/io/fs.py:229: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([GlobalStorage])` to allowlist this global.\n",
      "  warnings.warn(f\"{warn_msg} Please use \"\n",
      "/Users/naveenmalla/Documents/Projects/Thesis/Code/Synthetic_City_Generator/venv/lib/python3.12/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Planetoid(\"vgae_data\", \"CiteSeer\", transform=T.NormalizeFeatures())\n",
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "data.train_mask = data.val_mask = data.test_mask = None\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naveenmalla/Documents/Projects/Thesis/Code/Synthetic_City_Generator/venv/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "data = train_test_split_edges(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], y=[3327], val_pos_edge_index=[2, 227], test_pos_edge_index=[2, 455], train_pos_edge_index=[2, 7740], train_neg_adj_mask=[3327, 3327], val_neg_edge_index=[2, 227], test_neg_edge_index=[2, 455])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GCNEncoder(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(GCNEncoder, self).__init__()\n",
    "#         self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True) # cached only for transductive learning\n",
    "#         self.conv2 = GCNConv(2 * out_channels, out_channels, cached=True) # cached only for transductive learning\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = self.conv1(x, edge_index).relu()\n",
    "#         return self.conv2(x, edge_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.nn import GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parameters\n",
    "# out_channels = 2\n",
    "# num_features = dataset.num_features\n",
    "# epochs = 100\n",
    "\n",
    "# # model\n",
    "# model = GAE(GCNEncoder(num_features, out_channels))\n",
    "\n",
    "# # move to GPU (if available)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)\n",
    "# x = data.x.to(device)\n",
    "# train_pos_edge_index = data.train_pos_edge_index.to(device)\n",
    "\n",
    "# # inizialize the optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train():\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     z = model.encode(x, train_pos_edge_index)\n",
    "#     loss = model.recon_loss(z, train_pos_edge_index)\n",
    "#     #if args.variational:\n",
    "#     #   loss = loss + (1 / data.num_nodes) * model.kl_loss()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return float(loss)\n",
    "\n",
    "\n",
    "# def test(pos_edge_index, neg_edge_index):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         z = model.encode(x, train_pos_edge_index)\n",
    "#     return model.test(z, pos_edge_index, neg_edge_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for epoch in range(1, epochs + 1):\n",
    "#     loss = train()\n",
    "\n",
    "#     auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "#     print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z = model.encode(x, train_pos_edge_index)\n",
    "# Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are the results (AUC) and (AP) easy to read and compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in ./venv/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./venv/lib/python3.12/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./venv/lib/python3.12/site-packages (from tensorboard) (1.67.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.12/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./venv/lib/python3.12/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from tensorboard) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in ./venv/lib/python3.12/site-packages (from tensorboard) (5.28.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./venv/lib/python3.12/site-packages (from tensorboard) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in ./venv/lib/python3.12/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venv/lib/python3.12/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.12/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parameters\n",
    "# out_channels = 2\n",
    "# num_features = dataset.num_features\n",
    "# epochs = 100\n",
    "\n",
    "# # model\n",
    "# model = GAE(GCNEncoder(num_features, out_channels))\n",
    "\n",
    "# # move to GPU (if available)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)\n",
    "# x = data.x.to(device)\n",
    "# train_pos_edge_index = data.train_pos_edge_index.to(device)\n",
    "\n",
    "# # inizialize the optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import tensorboard\n",
    "\n",
    "#### Installation: (if needed) \"pip install tensorboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = SummaryWriter('runs/GAE1_experiment_'+'2d_100_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for epoch in range(1, epochs + 1):\n",
    "#     loss = train()\n",
    "#     auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "#     print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "    \n",
    "    \n",
    "#     writer.add_scalar('auc train',auc,epoch) # new line\n",
    "#     writer.add_scalar('ap train',ap,epoch)   # new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Variational AutoEncoder (GVAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import VGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(\"vgae_data\", \"CiteSeer\", transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
    "data = train_test_split_edges(data)\n",
    "\n",
    "\n",
    "class VariationalGCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(VariationalGCNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True) # cached only for transductive learning\n",
    "        self.conv_mu = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "        self.conv_logstd = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_channels = 2\n",
    "num_features = dataset.num_features\n",
    "epochs = 300\n",
    "\n",
    "\n",
    "model = VGAE(VariationalGCNEncoder(num_features, out_channels))  # new line\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "x = data.x.to(device)\n",
    "train_pos_edge_index = data.train_pos_edge_index.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x, train_pos_edge_index)\n",
    "    loss = model.recon_loss(z, train_pos_edge_index)\n",
    "    \n",
    "    loss = loss + (1 / data.num_nodes) * model.kl_loss()  # new line\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, train_pos_edge_index)\n",
    "    return model.test(z, pos_edge_index, neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, AUC: 0.4761, AP: 0.5369\n",
      "Epoch: 002, AUC: 0.5441, AP: 0.5939\n",
      "Epoch: 003, AUC: 0.6506, AP: 0.6981\n",
      "Epoch: 004, AUC: 0.6735, AP: 0.7088\n",
      "Epoch: 005, AUC: 0.6737, AP: 0.7066\n",
      "Epoch: 006, AUC: 0.6673, AP: 0.7002\n",
      "Epoch: 007, AUC: 0.6630, AP: 0.6965\n",
      "Epoch: 008, AUC: 0.6615, AP: 0.6947\n",
      "Epoch: 009, AUC: 0.6622, AP: 0.6945\n",
      "Epoch: 010, AUC: 0.6571, AP: 0.6899\n",
      "Epoch: 011, AUC: 0.6436, AP: 0.6811\n",
      "Epoch: 012, AUC: 0.6298, AP: 0.6713\n",
      "Epoch: 013, AUC: 0.5959, AP: 0.6501\n",
      "Epoch: 014, AUC: 0.5632, AP: 0.6298\n",
      "Epoch: 015, AUC: 0.5402, AP: 0.6094\n",
      "Epoch: 016, AUC: 0.5239, AP: 0.5924\n",
      "Epoch: 017, AUC: 0.5361, AP: 0.6039\n",
      "Epoch: 018, AUC: 0.5637, AP: 0.6338\n",
      "Epoch: 019, AUC: 0.5750, AP: 0.6353\n",
      "Epoch: 020, AUC: 0.5748, AP: 0.6283\n",
      "Epoch: 021, AUC: 0.5716, AP: 0.6198\n",
      "Epoch: 022, AUC: 0.5741, AP: 0.6197\n",
      "Epoch: 023, AUC: 0.5850, AP: 0.6268\n",
      "Epoch: 024, AUC: 0.6020, AP: 0.6394\n",
      "Epoch: 025, AUC: 0.6107, AP: 0.6473\n",
      "Epoch: 026, AUC: 0.6192, AP: 0.6549\n",
      "Epoch: 027, AUC: 0.6284, AP: 0.6624\n",
      "Epoch: 028, AUC: 0.6356, AP: 0.6681\n",
      "Epoch: 029, AUC: 0.6434, AP: 0.6754\n",
      "Epoch: 030, AUC: 0.6501, AP: 0.6826\n",
      "Epoch: 031, AUC: 0.6547, AP: 0.6880\n",
      "Epoch: 032, AUC: 0.6580, AP: 0.6915\n",
      "Epoch: 033, AUC: 0.6596, AP: 0.6939\n",
      "Epoch: 034, AUC: 0.6616, AP: 0.6967\n",
      "Epoch: 035, AUC: 0.6620, AP: 0.6984\n",
      "Epoch: 036, AUC: 0.6614, AP: 0.6992\n",
      "Epoch: 037, AUC: 0.6612, AP: 0.6998\n",
      "Epoch: 038, AUC: 0.6609, AP: 0.7005\n",
      "Epoch: 039, AUC: 0.6609, AP: 0.7014\n",
      "Epoch: 040, AUC: 0.6611, AP: 0.7016\n",
      "Epoch: 041, AUC: 0.6615, AP: 0.7023\n",
      "Epoch: 042, AUC: 0.6620, AP: 0.7029\n",
      "Epoch: 043, AUC: 0.6624, AP: 0.7029\n",
      "Epoch: 044, AUC: 0.6625, AP: 0.7031\n",
      "Epoch: 045, AUC: 0.6627, AP: 0.7032\n",
      "Epoch: 046, AUC: 0.6629, AP: 0.7032\n",
      "Epoch: 047, AUC: 0.6630, AP: 0.7033\n",
      "Epoch: 048, AUC: 0.6634, AP: 0.7038\n",
      "Epoch: 049, AUC: 0.6637, AP: 0.7042\n",
      "Epoch: 050, AUC: 0.6640, AP: 0.7045\n",
      "Epoch: 051, AUC: 0.6643, AP: 0.7049\n",
      "Epoch: 052, AUC: 0.6648, AP: 0.7055\n",
      "Epoch: 053, AUC: 0.6653, AP: 0.7062\n",
      "Epoch: 054, AUC: 0.6659, AP: 0.7071\n",
      "Epoch: 055, AUC: 0.6665, AP: 0.7078\n",
      "Epoch: 056, AUC: 0.6672, AP: 0.7087\n",
      "Epoch: 057, AUC: 0.6676, AP: 0.7094\n",
      "Epoch: 058, AUC: 0.6682, AP: 0.7102\n",
      "Epoch: 059, AUC: 0.6687, AP: 0.7111\n",
      "Epoch: 060, AUC: 0.6696, AP: 0.7121\n",
      "Epoch: 061, AUC: 0.6703, AP: 0.7131\n",
      "Epoch: 062, AUC: 0.6712, AP: 0.7145\n",
      "Epoch: 063, AUC: 0.6719, AP: 0.7157\n",
      "Epoch: 064, AUC: 0.6725, AP: 0.7167\n",
      "Epoch: 065, AUC: 0.6733, AP: 0.7181\n",
      "Epoch: 066, AUC: 0.6744, AP: 0.7198\n",
      "Epoch: 067, AUC: 0.6750, AP: 0.7211\n",
      "Epoch: 068, AUC: 0.6754, AP: 0.7223\n",
      "Epoch: 069, AUC: 0.6758, AP: 0.7233\n",
      "Epoch: 070, AUC: 0.6766, AP: 0.7246\n",
      "Epoch: 071, AUC: 0.6770, AP: 0.7258\n",
      "Epoch: 072, AUC: 0.6776, AP: 0.7271\n",
      "Epoch: 073, AUC: 0.6780, AP: 0.7281\n",
      "Epoch: 074, AUC: 0.6784, AP: 0.7291\n",
      "Epoch: 075, AUC: 0.6787, AP: 0.7299\n",
      "Epoch: 076, AUC: 0.6790, AP: 0.7308\n",
      "Epoch: 077, AUC: 0.6788, AP: 0.7317\n",
      "Epoch: 078, AUC: 0.6786, AP: 0.7326\n",
      "Epoch: 079, AUC: 0.6786, AP: 0.7333\n",
      "Epoch: 080, AUC: 0.6782, AP: 0.7340\n",
      "Epoch: 081, AUC: 0.6778, AP: 0.7348\n",
      "Epoch: 082, AUC: 0.6774, AP: 0.7355\n",
      "Epoch: 083, AUC: 0.6770, AP: 0.7362\n",
      "Epoch: 084, AUC: 0.6764, AP: 0.7368\n",
      "Epoch: 085, AUC: 0.6755, AP: 0.7372\n",
      "Epoch: 086, AUC: 0.6747, AP: 0.7375\n",
      "Epoch: 087, AUC: 0.6740, AP: 0.7379\n",
      "Epoch: 088, AUC: 0.6730, AP: 0.7381\n",
      "Epoch: 089, AUC: 0.6723, AP: 0.7384\n",
      "Epoch: 090, AUC: 0.6718, AP: 0.7389\n",
      "Epoch: 091, AUC: 0.6715, AP: 0.7390\n",
      "Epoch: 092, AUC: 0.6715, AP: 0.7393\n",
      "Epoch: 093, AUC: 0.6717, AP: 0.7398\n",
      "Epoch: 094, AUC: 0.6719, AP: 0.7401\n",
      "Epoch: 095, AUC: 0.6726, AP: 0.7407\n",
      "Epoch: 096, AUC: 0.6732, AP: 0.7406\n",
      "Epoch: 097, AUC: 0.6741, AP: 0.7410\n",
      "Epoch: 098, AUC: 0.6757, AP: 0.7416\n",
      "Epoch: 099, AUC: 0.6780, AP: 0.7422\n",
      "Epoch: 100, AUC: 0.6794, AP: 0.7424\n",
      "Epoch: 101, AUC: 0.6812, AP: 0.7425\n",
      "Epoch: 102, AUC: 0.6838, AP: 0.7431\n",
      "Epoch: 103, AUC: 0.6875, AP: 0.7441\n",
      "Epoch: 104, AUC: 0.6934, AP: 0.7464\n",
      "Epoch: 105, AUC: 0.7002, AP: 0.7491\n",
      "Epoch: 106, AUC: 0.7070, AP: 0.7523\n",
      "Epoch: 107, AUC: 0.7162, AP: 0.7569\n",
      "Epoch: 108, AUC: 0.7251, AP: 0.7619\n",
      "Epoch: 109, AUC: 0.7331, AP: 0.7664\n",
      "Epoch: 110, AUC: 0.7405, AP: 0.7707\n",
      "Epoch: 111, AUC: 0.7460, AP: 0.7740\n",
      "Epoch: 112, AUC: 0.7534, AP: 0.7787\n",
      "Epoch: 113, AUC: 0.7597, AP: 0.7830\n",
      "Epoch: 114, AUC: 0.7668, AP: 0.7877\n",
      "Epoch: 115, AUC: 0.7710, AP: 0.7905\n",
      "Epoch: 116, AUC: 0.7747, AP: 0.7932\n",
      "Epoch: 117, AUC: 0.7774, AP: 0.7953\n",
      "Epoch: 118, AUC: 0.7795, AP: 0.7970\n",
      "Epoch: 119, AUC: 0.7810, AP: 0.7983\n",
      "Epoch: 120, AUC: 0.7823, AP: 0.7993\n",
      "Epoch: 121, AUC: 0.7832, AP: 0.8001\n",
      "Epoch: 122, AUC: 0.7838, AP: 0.8007\n",
      "Epoch: 123, AUC: 0.7847, AP: 0.8015\n",
      "Epoch: 124, AUC: 0.7856, AP: 0.8024\n",
      "Epoch: 125, AUC: 0.7864, AP: 0.8030\n",
      "Epoch: 126, AUC: 0.7873, AP: 0.8038\n",
      "Epoch: 127, AUC: 0.7881, AP: 0.8044\n",
      "Epoch: 128, AUC: 0.7886, AP: 0.8048\n",
      "Epoch: 129, AUC: 0.7892, AP: 0.8052\n",
      "Epoch: 130, AUC: 0.7897, AP: 0.8058\n",
      "Epoch: 131, AUC: 0.7893, AP: 0.8056\n",
      "Epoch: 132, AUC: 0.7895, AP: 0.8060\n",
      "Epoch: 133, AUC: 0.7895, AP: 0.8061\n",
      "Epoch: 134, AUC: 0.7900, AP: 0.8065\n",
      "Epoch: 135, AUC: 0.7907, AP: 0.8071\n",
      "Epoch: 136, AUC: 0.7911, AP: 0.8070\n",
      "Epoch: 137, AUC: 0.7911, AP: 0.8071\n",
      "Epoch: 138, AUC: 0.7916, AP: 0.8078\n",
      "Epoch: 139, AUC: 0.7919, AP: 0.8084\n",
      "Epoch: 140, AUC: 0.7921, AP: 0.8088\n",
      "Epoch: 141, AUC: 0.7920, AP: 0.8086\n",
      "Epoch: 142, AUC: 0.7923, AP: 0.8089\n",
      "Epoch: 143, AUC: 0.7926, AP: 0.8087\n",
      "Epoch: 144, AUC: 0.7930, AP: 0.8088\n",
      "Epoch: 145, AUC: 0.7933, AP: 0.8090\n",
      "Epoch: 146, AUC: 0.7932, AP: 0.8092\n",
      "Epoch: 147, AUC: 0.7929, AP: 0.8089\n",
      "Epoch: 148, AUC: 0.7920, AP: 0.8081\n",
      "Epoch: 149, AUC: 0.7920, AP: 0.8079\n",
      "Epoch: 150, AUC: 0.7928, AP: 0.8084\n",
      "Epoch: 151, AUC: 0.7933, AP: 0.8083\n",
      "Epoch: 152, AUC: 0.7932, AP: 0.8081\n",
      "Epoch: 153, AUC: 0.7931, AP: 0.8079\n",
      "Epoch: 154, AUC: 0.7924, AP: 0.8072\n",
      "Epoch: 155, AUC: 0.7915, AP: 0.8065\n",
      "Epoch: 156, AUC: 0.7919, AP: 0.8064\n",
      "Epoch: 157, AUC: 0.7922, AP: 0.8067\n",
      "Epoch: 158, AUC: 0.7924, AP: 0.8063\n",
      "Epoch: 159, AUC: 0.7923, AP: 0.8059\n",
      "Epoch: 160, AUC: 0.7921, AP: 0.8056\n",
      "Epoch: 161, AUC: 0.7922, AP: 0.8055\n",
      "Epoch: 162, AUC: 0.7922, AP: 0.8055\n",
      "Epoch: 163, AUC: 0.7921, AP: 0.8054\n",
      "Epoch: 164, AUC: 0.7918, AP: 0.8049\n",
      "Epoch: 165, AUC: 0.7923, AP: 0.8049\n",
      "Epoch: 166, AUC: 0.7926, AP: 0.8050\n",
      "Epoch: 167, AUC: 0.7927, AP: 0.8048\n",
      "Epoch: 168, AUC: 0.7926, AP: 0.8047\n",
      "Epoch: 169, AUC: 0.7924, AP: 0.8045\n",
      "Epoch: 170, AUC: 0.7922, AP: 0.8043\n",
      "Epoch: 171, AUC: 0.7920, AP: 0.8039\n",
      "Epoch: 172, AUC: 0.7919, AP: 0.8038\n",
      "Epoch: 173, AUC: 0.7920, AP: 0.8039\n",
      "Epoch: 174, AUC: 0.7919, AP: 0.8037\n",
      "Epoch: 175, AUC: 0.7920, AP: 0.8039\n",
      "Epoch: 176, AUC: 0.7919, AP: 0.8038\n",
      "Epoch: 177, AUC: 0.7919, AP: 0.8035\n",
      "Epoch: 178, AUC: 0.7917, AP: 0.8034\n",
      "Epoch: 179, AUC: 0.7916, AP: 0.8033\n",
      "Epoch: 180, AUC: 0.7918, AP: 0.8037\n",
      "Epoch: 181, AUC: 0.7919, AP: 0.8037\n",
      "Epoch: 182, AUC: 0.7918, AP: 0.8037\n",
      "Epoch: 183, AUC: 0.7917, AP: 0.8037\n",
      "Epoch: 184, AUC: 0.7920, AP: 0.8040\n",
      "Epoch: 185, AUC: 0.7917, AP: 0.8036\n",
      "Epoch: 186, AUC: 0.7915, AP: 0.8035\n",
      "Epoch: 187, AUC: 0.7914, AP: 0.8035\n",
      "Epoch: 188, AUC: 0.7913, AP: 0.8035\n",
      "Epoch: 189, AUC: 0.7912, AP: 0.8034\n",
      "Epoch: 190, AUC: 0.7908, AP: 0.8031\n",
      "Epoch: 191, AUC: 0.7905, AP: 0.8029\n",
      "Epoch: 192, AUC: 0.7904, AP: 0.8031\n",
      "Epoch: 193, AUC: 0.7901, AP: 0.8028\n",
      "Epoch: 194, AUC: 0.7896, AP: 0.8024\n",
      "Epoch: 195, AUC: 0.7891, AP: 0.8021\n",
      "Epoch: 196, AUC: 0.7887, AP: 0.8019\n",
      "Epoch: 197, AUC: 0.7885, AP: 0.8019\n",
      "Epoch: 198, AUC: 0.7884, AP: 0.8016\n",
      "Epoch: 199, AUC: 0.7884, AP: 0.8018\n",
      "Epoch: 200, AUC: 0.7880, AP: 0.8016\n",
      "Epoch: 201, AUC: 0.7879, AP: 0.8015\n",
      "Epoch: 202, AUC: 0.7876, AP: 0.8013\n",
      "Epoch: 203, AUC: 0.7872, AP: 0.8010\n",
      "Epoch: 204, AUC: 0.7871, AP: 0.8008\n",
      "Epoch: 205, AUC: 0.7870, AP: 0.8008\n",
      "Epoch: 206, AUC: 0.7867, AP: 0.8006\n",
      "Epoch: 207, AUC: 0.7866, AP: 0.8005\n",
      "Epoch: 208, AUC: 0.7862, AP: 0.8001\n",
      "Epoch: 209, AUC: 0.7863, AP: 0.8003\n",
      "Epoch: 210, AUC: 0.7861, AP: 0.7999\n",
      "Epoch: 211, AUC: 0.7860, AP: 0.7998\n",
      "Epoch: 212, AUC: 0.7857, AP: 0.7998\n",
      "Epoch: 213, AUC: 0.7854, AP: 0.7996\n",
      "Epoch: 214, AUC: 0.7851, AP: 0.7994\n",
      "Epoch: 215, AUC: 0.7851, AP: 0.7992\n",
      "Epoch: 216, AUC: 0.7849, AP: 0.7990\n",
      "Epoch: 217, AUC: 0.7846, AP: 0.7985\n",
      "Epoch: 218, AUC: 0.7845, AP: 0.7985\n",
      "Epoch: 219, AUC: 0.7842, AP: 0.7984\n",
      "Epoch: 220, AUC: 0.7841, AP: 0.7985\n",
      "Epoch: 221, AUC: 0.7838, AP: 0.7982\n",
      "Epoch: 222, AUC: 0.7836, AP: 0.7978\n",
      "Epoch: 223, AUC: 0.7836, AP: 0.7973\n",
      "Epoch: 224, AUC: 0.7836, AP: 0.7974\n",
      "Epoch: 225, AUC: 0.7833, AP: 0.7973\n",
      "Epoch: 226, AUC: 0.7825, AP: 0.7971\n",
      "Epoch: 227, AUC: 0.7820, AP: 0.7966\n",
      "Epoch: 228, AUC: 0.7820, AP: 0.7966\n",
      "Epoch: 229, AUC: 0.7826, AP: 0.7966\n",
      "Epoch: 230, AUC: 0.7823, AP: 0.7960\n",
      "Epoch: 231, AUC: 0.7823, AP: 0.7962\n",
      "Epoch: 232, AUC: 0.7816, AP: 0.7964\n",
      "Epoch: 233, AUC: 0.7812, AP: 0.7958\n",
      "Epoch: 234, AUC: 0.7812, AP: 0.7960\n",
      "Epoch: 235, AUC: 0.7816, AP: 0.7960\n",
      "Epoch: 236, AUC: 0.7815, AP: 0.7953\n",
      "Epoch: 237, AUC: 0.7813, AP: 0.7950\n",
      "Epoch: 238, AUC: 0.7813, AP: 0.7954\n",
      "Epoch: 239, AUC: 0.7803, AP: 0.7951\n",
      "Epoch: 240, AUC: 0.7802, AP: 0.7950\n",
      "Epoch: 241, AUC: 0.7801, AP: 0.7949\n",
      "Epoch: 242, AUC: 0.7803, AP: 0.7948\n",
      "Epoch: 243, AUC: 0.7803, AP: 0.7945\n",
      "Epoch: 244, AUC: 0.7801, AP: 0.7943\n",
      "Epoch: 245, AUC: 0.7797, AP: 0.7943\n",
      "Epoch: 246, AUC: 0.7795, AP: 0.7941\n",
      "Epoch: 247, AUC: 0.7794, AP: 0.7939\n",
      "Epoch: 248, AUC: 0.7794, AP: 0.7940\n",
      "Epoch: 249, AUC: 0.7792, AP: 0.7936\n",
      "Epoch: 250, AUC: 0.7793, AP: 0.7934\n",
      "Epoch: 251, AUC: 0.7791, AP: 0.7930\n",
      "Epoch: 252, AUC: 0.7787, AP: 0.7928\n",
      "Epoch: 253, AUC: 0.7785, AP: 0.7927\n",
      "Epoch: 254, AUC: 0.7784, AP: 0.7925\n",
      "Epoch: 255, AUC: 0.7784, AP: 0.7924\n",
      "Epoch: 256, AUC: 0.7784, AP: 0.7923\n",
      "Epoch: 257, AUC: 0.7783, AP: 0.7921\n",
      "Epoch: 258, AUC: 0.7781, AP: 0.7920\n",
      "Epoch: 259, AUC: 0.7779, AP: 0.7920\n",
      "Epoch: 260, AUC: 0.7773, AP: 0.7917\n",
      "Epoch: 261, AUC: 0.7770, AP: 0.7916\n",
      "Epoch: 262, AUC: 0.7776, AP: 0.7918\n",
      "Epoch: 263, AUC: 0.7777, AP: 0.7914\n",
      "Epoch: 264, AUC: 0.7776, AP: 0.7914\n",
      "Epoch: 265, AUC: 0.7774, AP: 0.7913\n",
      "Epoch: 266, AUC: 0.7766, AP: 0.7911\n",
      "Epoch: 267, AUC: 0.7761, AP: 0.7907\n",
      "Epoch: 268, AUC: 0.7762, AP: 0.7910\n",
      "Epoch: 269, AUC: 0.7766, AP: 0.7910\n",
      "Epoch: 270, AUC: 0.7766, AP: 0.7907\n",
      "Epoch: 271, AUC: 0.7764, AP: 0.7904\n",
      "Epoch: 272, AUC: 0.7759, AP: 0.7901\n",
      "Epoch: 273, AUC: 0.7752, AP: 0.7898\n",
      "Epoch: 274, AUC: 0.7750, AP: 0.7896\n",
      "Epoch: 275, AUC: 0.7754, AP: 0.7897\n",
      "Epoch: 276, AUC: 0.7755, AP: 0.7895\n",
      "Epoch: 277, AUC: 0.7754, AP: 0.7892\n",
      "Epoch: 278, AUC: 0.7753, AP: 0.7892\n",
      "Epoch: 279, AUC: 0.7747, AP: 0.7891\n",
      "Epoch: 280, AUC: 0.7739, AP: 0.7887\n",
      "Epoch: 281, AUC: 0.7742, AP: 0.7886\n",
      "Epoch: 282, AUC: 0.7746, AP: 0.7886\n",
      "Epoch: 283, AUC: 0.7749, AP: 0.7886\n",
      "Epoch: 284, AUC: 0.7747, AP: 0.7882\n",
      "Epoch: 285, AUC: 0.7746, AP: 0.7884\n",
      "Epoch: 286, AUC: 0.7743, AP: 0.7881\n",
      "Epoch: 287, AUC: 0.7739, AP: 0.7880\n",
      "Epoch: 288, AUC: 0.7740, AP: 0.7879\n",
      "Epoch: 289, AUC: 0.7742, AP: 0.7878\n",
      "Epoch: 290, AUC: 0.7742, AP: 0.7878\n",
      "Epoch: 291, AUC: 0.7739, AP: 0.7876\n",
      "Epoch: 292, AUC: 0.7736, AP: 0.7875\n",
      "Epoch: 293, AUC: 0.7735, AP: 0.7874\n",
      "Epoch: 294, AUC: 0.7736, AP: 0.7873\n",
      "Epoch: 295, AUC: 0.7735, AP: 0.7873\n",
      "Epoch: 296, AUC: 0.7736, AP: 0.7874\n",
      "Epoch: 297, AUC: 0.7733, AP: 0.7873\n",
      "Epoch: 298, AUC: 0.7732, AP: 0.7871\n",
      "Epoch: 299, AUC: 0.7732, AP: 0.7872\n",
      "Epoch: 300, AUC: 0.7731, AP: 0.7872\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('runs/VGAE_experiment_'+'2d_100_epochs')\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train()\n",
    "    auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "    print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "    \n",
    "    \n",
    "    writer.add_scalar('auc train',auc,epoch) # new line\n",
    "    writer.add_scalar('ap train',ap,epoch)   # new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naveenmalla/Documents/Projects/Thesis/Code/Synthetic_City_Generator/venv/lib/python3.12/site-packages/torch_geometric/io/fs.py:229: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([GlobalStorage])` to allowlist this global.\n",
      "  warnings.warn(f\"{warn_msg} Please use \"\n",
      "/Users/naveenmalla/Documents/Projects/Thesis/Code/Synthetic_City_Generator/venv/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, AUC: 0.6082, AP: 0.6459\n",
      "Epoch: 002, AUC: 0.6303, AP: 0.6656\n",
      "Epoch: 003, AUC: 0.6496, AP: 0.6771\n",
      "Epoch: 004, AUC: 0.6535, AP: 0.6789\n",
      "Epoch: 005, AUC: 0.6554, AP: 0.6786\n",
      "Epoch: 006, AUC: 0.6560, AP: 0.6781\n",
      "Epoch: 007, AUC: 0.6564, AP: 0.6777\n",
      "Epoch: 008, AUC: 0.6567, AP: 0.6772\n",
      "Epoch: 009, AUC: 0.6568, AP: 0.6768\n",
      "Epoch: 010, AUC: 0.6565, AP: 0.6764\n",
      "Epoch: 011, AUC: 0.6563, AP: 0.6762\n",
      "Epoch: 012, AUC: 0.6560, AP: 0.6763\n",
      "Epoch: 013, AUC: 0.6554, AP: 0.6754\n",
      "Epoch: 014, AUC: 0.6551, AP: 0.6749\n",
      "Epoch: 015, AUC: 0.6546, AP: 0.6745\n",
      "Epoch: 016, AUC: 0.6544, AP: 0.6747\n",
      "Epoch: 017, AUC: 0.6539, AP: 0.6744\n",
      "Epoch: 018, AUC: 0.6537, AP: 0.6745\n",
      "Epoch: 019, AUC: 0.6534, AP: 0.6741\n",
      "Epoch: 020, AUC: 0.6530, AP: 0.6735\n",
      "Epoch: 021, AUC: 0.6524, AP: 0.6735\n",
      "Epoch: 022, AUC: 0.6518, AP: 0.6727\n",
      "Epoch: 023, AUC: 0.6505, AP: 0.6718\n",
      "Epoch: 024, AUC: 0.6492, AP: 0.6710\n",
      "Epoch: 025, AUC: 0.6477, AP: 0.6704\n",
      "Epoch: 026, AUC: 0.6464, AP: 0.6696\n",
      "Epoch: 027, AUC: 0.6453, AP: 0.6693\n",
      "Epoch: 028, AUC: 0.6446, AP: 0.6691\n",
      "Epoch: 029, AUC: 0.6452, AP: 0.6699\n",
      "Epoch: 030, AUC: 0.6460, AP: 0.6703\n",
      "Epoch: 031, AUC: 0.6483, AP: 0.6722\n",
      "Epoch: 032, AUC: 0.6506, AP: 0.6741\n",
      "Epoch: 033, AUC: 0.6535, AP: 0.6766\n",
      "Epoch: 034, AUC: 0.6576, AP: 0.6803\n",
      "Epoch: 035, AUC: 0.6598, AP: 0.6827\n",
      "Epoch: 036, AUC: 0.6616, AP: 0.6845\n",
      "Epoch: 037, AUC: 0.6629, AP: 0.6859\n",
      "Epoch: 038, AUC: 0.6646, AP: 0.6879\n",
      "Epoch: 039, AUC: 0.6657, AP: 0.6891\n",
      "Epoch: 040, AUC: 0.6668, AP: 0.6899\n",
      "Epoch: 041, AUC: 0.6675, AP: 0.6906\n",
      "Epoch: 042, AUC: 0.6683, AP: 0.6917\n",
      "Epoch: 043, AUC: 0.6691, AP: 0.6922\n",
      "Epoch: 044, AUC: 0.6699, AP: 0.6928\n",
      "Epoch: 045, AUC: 0.6709, AP: 0.6938\n",
      "Epoch: 046, AUC: 0.6721, AP: 0.6948\n",
      "Epoch: 047, AUC: 0.6732, AP: 0.6957\n",
      "Epoch: 048, AUC: 0.6742, AP: 0.6967\n",
      "Epoch: 049, AUC: 0.6753, AP: 0.6979\n",
      "Epoch: 050, AUC: 0.6765, AP: 0.6994\n",
      "Epoch: 051, AUC: 0.6780, AP: 0.7010\n",
      "Epoch: 052, AUC: 0.6793, AP: 0.7027\n",
      "Epoch: 053, AUC: 0.6809, AP: 0.7045\n",
      "Epoch: 054, AUC: 0.6826, AP: 0.7064\n",
      "Epoch: 055, AUC: 0.6842, AP: 0.7083\n",
      "Epoch: 056, AUC: 0.6855, AP: 0.7102\n",
      "Epoch: 057, AUC: 0.6867, AP: 0.7119\n",
      "Epoch: 058, AUC: 0.6879, AP: 0.7139\n",
      "Epoch: 059, AUC: 0.6891, AP: 0.7157\n",
      "Epoch: 060, AUC: 0.6898, AP: 0.7173\n",
      "Epoch: 061, AUC: 0.6906, AP: 0.7189\n",
      "Epoch: 062, AUC: 0.6911, AP: 0.7203\n",
      "Epoch: 063, AUC: 0.6915, AP: 0.7215\n",
      "Epoch: 064, AUC: 0.6923, AP: 0.7232\n",
      "Epoch: 065, AUC: 0.6927, AP: 0.7249\n",
      "Epoch: 066, AUC: 0.6931, AP: 0.7266\n",
      "Epoch: 067, AUC: 0.6928, AP: 0.7274\n",
      "Epoch: 068, AUC: 0.6933, AP: 0.7288\n",
      "Epoch: 069, AUC: 0.6936, AP: 0.7301\n",
      "Epoch: 070, AUC: 0.6939, AP: 0.7313\n",
      "Epoch: 071, AUC: 0.6936, AP: 0.7324\n",
      "Epoch: 072, AUC: 0.6936, AP: 0.7336\n",
      "Epoch: 073, AUC: 0.6936, AP: 0.7347\n",
      "Epoch: 074, AUC: 0.6936, AP: 0.7361\n",
      "Epoch: 075, AUC: 0.6937, AP: 0.7374\n",
      "Epoch: 076, AUC: 0.6937, AP: 0.7384\n",
      "Epoch: 077, AUC: 0.6937, AP: 0.7394\n",
      "Epoch: 078, AUC: 0.6938, AP: 0.7408\n",
      "Epoch: 079, AUC: 0.6940, AP: 0.7422\n",
      "Epoch: 080, AUC: 0.6943, AP: 0.7433\n",
      "Epoch: 081, AUC: 0.6947, AP: 0.7444\n",
      "Epoch: 082, AUC: 0.6947, AP: 0.7452\n",
      "Epoch: 083, AUC: 0.6949, AP: 0.7459\n",
      "Epoch: 084, AUC: 0.6954, AP: 0.7468\n",
      "Epoch: 085, AUC: 0.6959, AP: 0.7476\n",
      "Epoch: 086, AUC: 0.6965, AP: 0.7484\n",
      "Epoch: 087, AUC: 0.6972, AP: 0.7491\n",
      "Epoch: 088, AUC: 0.6982, AP: 0.7499\n",
      "Epoch: 089, AUC: 0.6996, AP: 0.7511\n",
      "Epoch: 090, AUC: 0.7012, AP: 0.7522\n",
      "Epoch: 091, AUC: 0.7035, AP: 0.7533\n",
      "Epoch: 092, AUC: 0.7068, AP: 0.7550\n",
      "Epoch: 093, AUC: 0.7109, AP: 0.7567\n",
      "Epoch: 094, AUC: 0.7155, AP: 0.7583\n",
      "Epoch: 095, AUC: 0.7184, AP: 0.7596\n",
      "Epoch: 096, AUC: 0.7208, AP: 0.7605\n",
      "Epoch: 097, AUC: 0.7233, AP: 0.7617\n",
      "Epoch: 098, AUC: 0.7256, AP: 0.7628\n",
      "Epoch: 099, AUC: 0.7273, AP: 0.7637\n",
      "Epoch: 100, AUC: 0.7289, AP: 0.7646\n",
      "Epoch: 101, AUC: 0.7310, AP: 0.7657\n",
      "Epoch: 102, AUC: 0.7353, AP: 0.7679\n",
      "Epoch: 103, AUC: 0.7397, AP: 0.7702\n",
      "Epoch: 104, AUC: 0.7453, AP: 0.7733\n",
      "Epoch: 105, AUC: 0.7493, AP: 0.7756\n",
      "Epoch: 106, AUC: 0.7536, AP: 0.7782\n",
      "Epoch: 107, AUC: 0.7573, AP: 0.7805\n",
      "Epoch: 108, AUC: 0.7612, AP: 0.7826\n",
      "Epoch: 109, AUC: 0.7643, AP: 0.7844\n",
      "Epoch: 110, AUC: 0.7664, AP: 0.7856\n",
      "Epoch: 111, AUC: 0.7682, AP: 0.7868\n",
      "Epoch: 112, AUC: 0.7704, AP: 0.7881\n",
      "Epoch: 113, AUC: 0.7719, AP: 0.7889\n",
      "Epoch: 114, AUC: 0.7738, AP: 0.7900\n",
      "Epoch: 115, AUC: 0.7756, AP: 0.7912\n",
      "Epoch: 116, AUC: 0.7775, AP: 0.7922\n",
      "Epoch: 117, AUC: 0.7789, AP: 0.7929\n",
      "Epoch: 118, AUC: 0.7803, AP: 0.7938\n",
      "Epoch: 119, AUC: 0.7812, AP: 0.7942\n",
      "Epoch: 120, AUC: 0.7827, AP: 0.7949\n",
      "Epoch: 121, AUC: 0.7839, AP: 0.7954\n",
      "Epoch: 122, AUC: 0.7849, AP: 0.7958\n",
      "Epoch: 123, AUC: 0.7858, AP: 0.7962\n",
      "Epoch: 124, AUC: 0.7866, AP: 0.7964\n",
      "Epoch: 125, AUC: 0.7868, AP: 0.7961\n",
      "Epoch: 126, AUC: 0.7872, AP: 0.7962\n",
      "Epoch: 127, AUC: 0.7879, AP: 0.7970\n",
      "Epoch: 128, AUC: 0.7884, AP: 0.7974\n",
      "Epoch: 129, AUC: 0.7887, AP: 0.7975\n",
      "Epoch: 130, AUC: 0.7891, AP: 0.7978\n",
      "Epoch: 131, AUC: 0.7891, AP: 0.7977\n",
      "Epoch: 132, AUC: 0.7894, AP: 0.7978\n",
      "Epoch: 133, AUC: 0.7899, AP: 0.7983\n",
      "Epoch: 134, AUC: 0.7904, AP: 0.7988\n",
      "Epoch: 135, AUC: 0.7911, AP: 0.7999\n",
      "Epoch: 136, AUC: 0.7911, AP: 0.8000\n",
      "Epoch: 137, AUC: 0.7910, AP: 0.7998\n",
      "Epoch: 138, AUC: 0.7906, AP: 0.7995\n",
      "Epoch: 139, AUC: 0.7906, AP: 0.7996\n",
      "Epoch: 140, AUC: 0.7910, AP: 0.8001\n",
      "Epoch: 141, AUC: 0.7912, AP: 0.8006\n",
      "Epoch: 142, AUC: 0.7915, AP: 0.8013\n",
      "Epoch: 143, AUC: 0.7913, AP: 0.8010\n",
      "Epoch: 144, AUC: 0.7910, AP: 0.8007\n",
      "Epoch: 145, AUC: 0.7904, AP: 0.8002\n",
      "Epoch: 146, AUC: 0.7903, AP: 0.8004\n",
      "Epoch: 147, AUC: 0.7909, AP: 0.8013\n",
      "Epoch: 148, AUC: 0.7910, AP: 0.8016\n",
      "Epoch: 149, AUC: 0.7907, AP: 0.8013\n",
      "Epoch: 150, AUC: 0.7905, AP: 0.8013\n",
      "Epoch: 151, AUC: 0.7901, AP: 0.8009\n",
      "Epoch: 152, AUC: 0.7905, AP: 0.8015\n",
      "Epoch: 153, AUC: 0.7908, AP: 0.8022\n",
      "Epoch: 154, AUC: 0.7908, AP: 0.8024\n",
      "Epoch: 155, AUC: 0.7902, AP: 0.8019\n",
      "Epoch: 156, AUC: 0.7895, AP: 0.8011\n",
      "Epoch: 157, AUC: 0.7889, AP: 0.8005\n",
      "Epoch: 158, AUC: 0.7888, AP: 0.8007\n",
      "Epoch: 159, AUC: 0.7892, AP: 0.8013\n",
      "Epoch: 160, AUC: 0.7895, AP: 0.8018\n",
      "Epoch: 161, AUC: 0.7897, AP: 0.8021\n",
      "Epoch: 162, AUC: 0.7894, AP: 0.8020\n",
      "Epoch: 163, AUC: 0.7892, AP: 0.8018\n",
      "Epoch: 164, AUC: 0.7891, AP: 0.8019\n",
      "Epoch: 165, AUC: 0.7891, AP: 0.8019\n",
      "Epoch: 166, AUC: 0.7890, AP: 0.8020\n",
      "Epoch: 167, AUC: 0.7889, AP: 0.8020\n",
      "Epoch: 168, AUC: 0.7888, AP: 0.8020\n",
      "Epoch: 169, AUC: 0.7884, AP: 0.8017\n",
      "Epoch: 170, AUC: 0.7884, AP: 0.8018\n",
      "Epoch: 171, AUC: 0.7882, AP: 0.8018\n",
      "Epoch: 172, AUC: 0.7881, AP: 0.8016\n",
      "Epoch: 173, AUC: 0.7877, AP: 0.8010\n",
      "Epoch: 174, AUC: 0.7878, AP: 0.8012\n",
      "Epoch: 175, AUC: 0.7878, AP: 0.8013\n",
      "Epoch: 176, AUC: 0.7879, AP: 0.8014\n",
      "Epoch: 177, AUC: 0.7881, AP: 0.8017\n",
      "Epoch: 178, AUC: 0.7876, AP: 0.8012\n",
      "Epoch: 179, AUC: 0.7871, AP: 0.8004\n",
      "Epoch: 180, AUC: 0.7865, AP: 0.7994\n",
      "Epoch: 181, AUC: 0.7860, AP: 0.7987\n",
      "Epoch: 182, AUC: 0.7865, AP: 0.7995\n",
      "Epoch: 183, AUC: 0.7866, AP: 0.8001\n",
      "Epoch: 184, AUC: 0.7868, AP: 0.8001\n",
      "Epoch: 185, AUC: 0.7866, AP: 0.7998\n",
      "Epoch: 186, AUC: 0.7860, AP: 0.7993\n",
      "Epoch: 187, AUC: 0.7855, AP: 0.7985\n",
      "Epoch: 188, AUC: 0.7847, AP: 0.7974\n",
      "Epoch: 189, AUC: 0.7847, AP: 0.7972\n",
      "Epoch: 190, AUC: 0.7844, AP: 0.7971\n",
      "Epoch: 191, AUC: 0.7845, AP: 0.7975\n",
      "Epoch: 192, AUC: 0.7845, AP: 0.7975\n",
      "Epoch: 193, AUC: 0.7847, AP: 0.7977\n",
      "Epoch: 194, AUC: 0.7843, AP: 0.7973\n",
      "Epoch: 195, AUC: 0.7837, AP: 0.7965\n",
      "Epoch: 196, AUC: 0.7831, AP: 0.7953\n",
      "Epoch: 197, AUC: 0.7828, AP: 0.7949\n",
      "Epoch: 198, AUC: 0.7830, AP: 0.7955\n",
      "Epoch: 199, AUC: 0.7834, AP: 0.7965\n",
      "Epoch: 200, AUC: 0.7834, AP: 0.7965\n",
      "Epoch: 201, AUC: 0.7832, AP: 0.7963\n",
      "Epoch: 202, AUC: 0.7831, AP: 0.7960\n",
      "Epoch: 203, AUC: 0.7825, AP: 0.7951\n",
      "Epoch: 204, AUC: 0.7823, AP: 0.7945\n",
      "Epoch: 205, AUC: 0.7823, AP: 0.7944\n",
      "Epoch: 206, AUC: 0.7827, AP: 0.7953\n",
      "Epoch: 207, AUC: 0.7826, AP: 0.7954\n",
      "Epoch: 208, AUC: 0.7825, AP: 0.7952\n",
      "Epoch: 209, AUC: 0.7822, AP: 0.7944\n",
      "Epoch: 210, AUC: 0.7819, AP: 0.7941\n",
      "Epoch: 211, AUC: 0.7820, AP: 0.7942\n",
      "Epoch: 212, AUC: 0.7821, AP: 0.7944\n",
      "Epoch: 213, AUC: 0.7819, AP: 0.7942\n",
      "Epoch: 214, AUC: 0.7820, AP: 0.7945\n",
      "Epoch: 215, AUC: 0.7816, AP: 0.7938\n",
      "Epoch: 216, AUC: 0.7814, AP: 0.7933\n",
      "Epoch: 217, AUC: 0.7810, AP: 0.7927\n",
      "Epoch: 218, AUC: 0.7808, AP: 0.7924\n",
      "Epoch: 219, AUC: 0.7808, AP: 0.7927\n",
      "Epoch: 220, AUC: 0.7807, AP: 0.7928\n",
      "Epoch: 221, AUC: 0.7806, AP: 0.7928\n",
      "Epoch: 222, AUC: 0.7801, AP: 0.7921\n",
      "Epoch: 223, AUC: 0.7799, AP: 0.7916\n",
      "Epoch: 224, AUC: 0.7798, AP: 0.7916\n",
      "Epoch: 225, AUC: 0.7796, AP: 0.7918\n",
      "Epoch: 226, AUC: 0.7794, AP: 0.7915\n",
      "Epoch: 227, AUC: 0.7792, AP: 0.7913\n",
      "Epoch: 228, AUC: 0.7790, AP: 0.7911\n",
      "Epoch: 229, AUC: 0.7787, AP: 0.7908\n",
      "Epoch: 230, AUC: 0.7785, AP: 0.7909\n",
      "Epoch: 231, AUC: 0.7784, AP: 0.7906\n",
      "Epoch: 232, AUC: 0.7779, AP: 0.7899\n",
      "Epoch: 233, AUC: 0.7775, AP: 0.7889\n",
      "Epoch: 234, AUC: 0.7773, AP: 0.7886\n",
      "Epoch: 235, AUC: 0.7769, AP: 0.7881\n",
      "Epoch: 236, AUC: 0.7768, AP: 0.7884\n",
      "Epoch: 237, AUC: 0.7773, AP: 0.7900\n",
      "Epoch: 238, AUC: 0.7771, AP: 0.7898\n",
      "Epoch: 239, AUC: 0.7763, AP: 0.7883\n",
      "Epoch: 240, AUC: 0.7757, AP: 0.7868\n",
      "Epoch: 241, AUC: 0.7753, AP: 0.7859\n",
      "Epoch: 242, AUC: 0.7756, AP: 0.7869\n",
      "Epoch: 243, AUC: 0.7760, AP: 0.7881\n",
      "Epoch: 244, AUC: 0.7761, AP: 0.7887\n",
      "Epoch: 245, AUC: 0.7759, AP: 0.7878\n",
      "Epoch: 246, AUC: 0.7753, AP: 0.7864\n",
      "Epoch: 247, AUC: 0.7748, AP: 0.7852\n",
      "Epoch: 248, AUC: 0.7747, AP: 0.7851\n",
      "Epoch: 249, AUC: 0.7754, AP: 0.7868\n",
      "Epoch: 250, AUC: 0.7753, AP: 0.7873\n",
      "Epoch: 251, AUC: 0.7751, AP: 0.7873\n",
      "Epoch: 252, AUC: 0.7749, AP: 0.7869\n",
      "Epoch: 253, AUC: 0.7741, AP: 0.7848\n",
      "Epoch: 254, AUC: 0.7735, AP: 0.7837\n",
      "Epoch: 255, AUC: 0.7737, AP: 0.7844\n",
      "Epoch: 256, AUC: 0.7741, AP: 0.7855\n",
      "Epoch: 257, AUC: 0.7740, AP: 0.7859\n",
      "Epoch: 258, AUC: 0.7737, AP: 0.7857\n",
      "Epoch: 259, AUC: 0.7734, AP: 0.7853\n",
      "Epoch: 260, AUC: 0.7729, AP: 0.7837\n",
      "Epoch: 261, AUC: 0.7724, AP: 0.7826\n",
      "Epoch: 262, AUC: 0.7725, AP: 0.7829\n",
      "Epoch: 263, AUC: 0.7728, AP: 0.7845\n",
      "Epoch: 264, AUC: 0.7728, AP: 0.7855\n",
      "Epoch: 265, AUC: 0.7727, AP: 0.7854\n",
      "Epoch: 266, AUC: 0.7724, AP: 0.7841\n",
      "Epoch: 267, AUC: 0.7720, AP: 0.7827\n",
      "Epoch: 268, AUC: 0.7720, AP: 0.7825\n",
      "Epoch: 269, AUC: 0.7716, AP: 0.7818\n",
      "Epoch: 270, AUC: 0.7718, AP: 0.7824\n",
      "Epoch: 271, AUC: 0.7721, AP: 0.7834\n",
      "Epoch: 272, AUC: 0.7722, AP: 0.7843\n",
      "Epoch: 273, AUC: 0.7723, AP: 0.7843\n",
      "Epoch: 274, AUC: 0.7719, AP: 0.7833\n",
      "Epoch: 275, AUC: 0.7717, AP: 0.7820\n",
      "Epoch: 276, AUC: 0.7713, AP: 0.7813\n",
      "Epoch: 277, AUC: 0.7712, AP: 0.7810\n",
      "Epoch: 278, AUC: 0.7714, AP: 0.7817\n",
      "Epoch: 279, AUC: 0.7714, AP: 0.7822\n",
      "Epoch: 280, AUC: 0.7713, AP: 0.7820\n",
      "Epoch: 281, AUC: 0.7711, AP: 0.7818\n",
      "Epoch: 282, AUC: 0.7709, AP: 0.7812\n",
      "Epoch: 283, AUC: 0.7708, AP: 0.7810\n",
      "Epoch: 284, AUC: 0.7705, AP: 0.7806\n",
      "Epoch: 285, AUC: 0.7703, AP: 0.7805\n",
      "Epoch: 286, AUC: 0.7706, AP: 0.7813\n",
      "Epoch: 287, AUC: 0.7706, AP: 0.7817\n",
      "Epoch: 288, AUC: 0.7705, AP: 0.7814\n",
      "Epoch: 289, AUC: 0.7705, AP: 0.7812\n",
      "Epoch: 290, AUC: 0.7705, AP: 0.7812\n",
      "Epoch: 291, AUC: 0.7703, AP: 0.7807\n",
      "Epoch: 292, AUC: 0.7702, AP: 0.7805\n",
      "Epoch: 293, AUC: 0.7705, AP: 0.7813\n",
      "Epoch: 294, AUC: 0.7707, AP: 0.7818\n",
      "Epoch: 295, AUC: 0.7706, AP: 0.7817\n",
      "Epoch: 296, AUC: 0.7701, AP: 0.7806\n",
      "Epoch: 297, AUC: 0.7694, AP: 0.7793\n",
      "Epoch: 298, AUC: 0.7692, AP: 0.7786\n",
      "Epoch: 299, AUC: 0.7695, AP: 0.7798\n",
      "Epoch: 300, AUC: 0.7698, AP: 0.7805\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import VGAE, GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Dataset preparation\n",
    "dataset = Planetoid(\"vgae_data\", \"CiteSeer\", transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
    "data = train_test_split_edges(data)\n",
    "\n",
    "# Define the Variational Graph Convolutional Network Encoder\n",
    "class VariationalGCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(VariationalGCNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True)  # cached only for transductive learning\n",
    "        self.conv_mu = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "        self.conv_logstd = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
    "\n",
    "# VGAE parameters\n",
    "out_channels = 2\n",
    "num_features = dataset.num_features\n",
    "epochs = 300\n",
    "\n",
    "# Model, device setup, and optimizer\n",
    "model = VGAE(VariationalGCNEncoder(num_features, out_channels))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "x = data.x.to(device)\n",
    "train_pos_edge_index = data.train_pos_edge_index.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training and testing functions\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x, train_pos_edge_index)\n",
    "    loss = model.recon_loss(z, train_pos_edge_index)\n",
    "    loss = loss + (1 / data.num_nodes) * model.kl_loss()  # KL divergence loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, train_pos_edge_index)\n",
    "    return model.test(z, pos_edge_index, neg_edge_index)\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter('runs/VGAE_experiment_' + '2d_100_epochs')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train()\n",
    "    auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "    print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "    \n",
    "    writer.add_scalar('auc train', auc, epoch)  # Log AUC\n",
    "    writer.add_scalar('ap train', ap, epoch)   # Log AP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
