{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers torch sentencepiece accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naveenmalla/Documents/Projects/Thesis/Code/Synthetic_City_Generator/venv/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                 \n",
      "  1%|          | 10/1830 [02:27<12:42,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.3745, 'grad_norm': 168.86695861816406, 'learning_rate': 4.965986394557823e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "  1%|          | 10/1830 [02:32<12:42,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.251, 'grad_norm': 41.83161544799805, 'learning_rate': 4.931972789115647e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "  1%|          | 10/1830 [02:36<12:42,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.788, 'grad_norm': 5.38971471786499, 'learning_rate': 4.89795918367347e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "  1%|          | 10/1830 [02:40<12:42,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2663, 'grad_norm': 4.641617298126221, 'learning_rate': 4.8639455782312926e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "  1%|          | 10/1830 [02:44<12:42,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0262, 'grad_norm': 3.3070597648620605, 'learning_rate': 4.8299319727891155e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "  1%|          | 10/1830 [02:47<12:42,  2.39it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5146335363388062, 'eval_runtime': 3.1573, 'eval_samples_per_second': 92.8, 'eval_steps_per_second': 11.719, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "  1%|          | 10/1830 [02:51<12:42,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8014, 'grad_norm': 2.6286284923553467, 'learning_rate': 4.795918367346939e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "  1%|          | 10/1830 [02:56<12:42,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6588, 'grad_norm': 10.684414863586426, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "  1%|          | 10/1830 [03:02<12:42,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5002, 'grad_norm': 1.4552953243255615, 'learning_rate': 4.7278911564625856e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "  1%|          | 10/1830 [03:08<12:42,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4799, 'grad_norm': 2.1467034816741943, 'learning_rate': 4.6938775510204086e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'quantized_coordinates.csv'\n",
    "coordinates_df = pd.read_csv(file_path)\n",
    "\n",
    "# Combine y_quant and x_quant into a sequence\n",
    "coordinates_df['sequence'] = coordinates_df['y_quant'].astype(str) + ' ' + coordinates_df['x_quant'].astype(str)\n",
    "\n",
    "# Prepare data: We'll create input-output pairs where the input is the first few coordinates, and the output is the full sequence.\n",
    "def prepare_data(df, input_len=3):\n",
    "    input_sequences = []\n",
    "    output_sequences = []\n",
    "    for i in range(len(df) - input_len):\n",
    "        input_seq = ' '.join(df['sequence'].iloc[i:i+input_len])\n",
    "        output_seq = ' '.join(df['sequence'].iloc[i:i+input_len+1])\n",
    "        input_sequences.append(input_seq)\n",
    "        output_sequences.append(output_seq)\n",
    "    return input_sequences, output_sequences\n",
    "\n",
    "input_seqs, output_seqs = prepare_data(coordinates_df)\n",
    "\n",
    "# Create a custom dataset class\n",
    "class CoordinateDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs, tokenizer, max_len):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.inputs[idx]\n",
    "        output_seq = self.outputs[idx]\n",
    "        \n",
    "        # Tokenize the input and output sequences\n",
    "        inputs = self.tokenizer(input_seq, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        outputs = self.tokenizer(output_seq, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs.input_ids.flatten(),\n",
    "            'attention_mask': inputs.attention_mask.flatten(),\n",
    "            'labels': outputs.input_ids.flatten()\n",
    "        }\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Dataset parameters\n",
    "MAX_LEN = 50\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "# Split the data into train and validation sets (80% train, 20% validation)\n",
    "train_inputs, val_inputs, train_outputs, val_outputs = train_test_split(\n",
    "    input_seqs, output_seqs, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create train and eval datasets\n",
    "train_dataset = CoordinateDataset(train_inputs, train_outputs, tokenizer, max_len=MAX_LEN)\n",
    "eval_dataset = CoordinateDataset(val_inputs, val_outputs, tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "# Training arguments (updated)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",  # Evaluate at each logging step\n",
    "    eval_steps=50,  # Evaluate every 50 steps\n",
    "    save_steps=500,  # Save model every 500 steps\n",
    ")\n",
    "\n",
    "# Initialize Trainer with evaluation dataset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # Add the evaluation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Inference: Generate a sequence given the first few points\n",
    "def generate_sequence(model, tokenizer, input_sequence, max_length=50):\n",
    "    inputs = tokenizer(input_sequence, return_tensors=\"pt\")\n",
    "    output = model.generate(inputs.input_ids, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example prediction\n",
    "input_sequence = '88 121 89 121 90'\n",
    "predicted_sequence = generate_sequence(model, tokenizer, input_sequence)\n",
    "print(f\"Input: {input_sequence}\")\n",
    "print(f\"Predicted Sequence: {predicted_sequence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
