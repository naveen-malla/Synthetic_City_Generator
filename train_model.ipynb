{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers torch sentencepiece accelerate protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "file_path = 'quantized_coordinates.csv'\n",
    "coordinates_df = pd.read_csv(file_path)\n",
    "coordinates_df['sequence'] = coordinates_df['y_quant'].astype(str) + ' ' + coordinates_df['x_quant'].astype(str)\n",
    "\n",
    "# Prepare input-output pairs for training\n",
    "def prepare_data(df, input_len=5):\n",
    "    input_sequences = []\n",
    "    output_sequences = []\n",
    "    for i in range(len(df) - input_len):\n",
    "        input_seq = ' '.join(df['sequence'].iloc[i:i+input_len])\n",
    "        output_seq = ' '.join(df['sequence'].iloc[i:i+input_len+1])\n",
    "        input_sequences.append(input_seq)\n",
    "        output_sequences.append(output_seq)\n",
    "    return input_sequences, output_sequences\n",
    "\n",
    "input_seqs, output_seqs = prepare_data(coordinates_df)\n",
    "\n",
    "# Split the data into train and validation sets (80% train, 20% validation)\n",
    "train_inputs, val_inputs, train_outputs, val_outputs = train_test_split(\n",
    "    input_seqs, output_seqs, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CoordinateDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs, tokenizer, max_len, device):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.inputs[idx]\n",
    "        output_seq = self.outputs[idx]\n",
    "        \n",
    "        inputs = self.tokenizer(input_seq, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        outputs = self.tokenizer(output_seq, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs.input_ids.flatten().to(self.device),\n",
    "            'attention_mask': inputs.attention_mask.flatten().to(self.device),\n",
    "            'labels': outputs.input_ids.flatten().to(self.device)\n",
    "        }\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model_name = \"t5-base\"  # Or 't5-small' for faster training\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Check if MPS is available and move model to MPS\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Dataset parameters\n",
    "MAX_LEN = 50  # Reduced sequence length for faster training\n",
    "BATCH_SIZE = 2  # Smaller batch size to fit in memory\n",
    "\n",
    "# Create train and eval datasets\n",
    "train_dataset = CoordinateDataset(train_inputs, train_outputs, tokenizer, max_len=MAX_LEN, device=device)\n",
    "eval_dataset = CoordinateDataset(val_inputs, val_outputs, tokenizer, max_len=MAX_LEN, device=device)\n",
    "\n",
    "# Training arguments without FP16\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,  # Start with fewer epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,  # Simulate larger batch size\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,  # Evaluation every 50 steps\n",
    "    save_steps=500,  # Save checkpoint every 500 steps\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inference: Generate a sequence given the first few points\n",
    "def generate_sequence(model, tokenizer, input_sequence, max_length=150):\n",
    "    inputs = tokenizer(input_sequence, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(inputs.input_ids, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example prediction\u001b[39;00m\n\u001b[1;32m      2\u001b[0m input_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m80 118 79 123 88 127\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m----> 3\u001b[0m predicted_sequence \u001b[38;5;241m=\u001b[39m generate_sequence(\u001b[43mmodel\u001b[49m, tokenizer, input_sequence)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_sequence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_sequence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Example prediction\n",
    "input_sequence = '80 118 79 123 88 127' \n",
    "predicted_sequence = generate_sequence(model, tokenizer, input_sequence)\n",
    "print(f\"Input: {input_sequence}\")\n",
    "print(f\"Predicted Sequence: {predicted_sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 80 118 79 123 88 127\n",
      "Predicted Sequence: 80 118 79 123 88 127 88 127\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('results/checkpoint-730')\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('results/checkpoint-730')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example prediction\n",
    "input_sequence = '80 118 79 123 88 127'\n",
    "inputs = tokenizer(input_sequence, return_tensors='pt')\n",
    "\n",
    "# Generate sequence\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=200,    # Increase max_length for longer output\n",
    "    num_beams=5,       # Use beam search with multiple beams for diverse results\n",
    "    repetition_penalty=1.2  # Penalize repetition\n",
    ")\n",
    "\n",
    "predicted_sequence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input: {input_sequence}\")\n",
    "print(f\"Predicted Sequence: {predicted_sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())  # Should return True on M1 Mac\n",
    "print(torch.backends.mps.is_built())      # Should return True if PyTorch was built with MPS support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
